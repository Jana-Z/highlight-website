<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Discover Highlight, a new method that enhances vision-language models by automatically learning visual prompts, improving image detection and localization tasks by 15%.">
  <meta property="og:title" content="Highlight: Learning Visual Prompts for Vision-Language Models"/>
  <!-- <meta property="og:description" content="Given an image collection and, optionally, image descriptions, we automatically learn a Highlight to prompt images. Highlight outperforms a red circle by 15% on the RefCOCO, RefCOCO+ and RefCOCOg datasets, on average."/>
  <meta property="og:url" content="https://jana-z.github.io/highlight-website/"/> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->

  <title>Highlight</title>

  <meta name="twitter:title" content="Highlight: Learning Visual Prompts for Vision-Language Models">
  <meta name="twitter:description" content="Discover Highlight, a new method that enhances vision-language models by automatically learning visual prompts, improving image detection and localization tasks by 15%.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/teaser.png"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="self-supervised learning, visual prompting">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title></title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Highlight: Learning Visual Prompts for Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="" target="_blank">Jana Zeller</a>,</span>
                <span class="author-block">
                  <a href="https://suny-sht.github.io/" target="_blank">Aleksandar (Suny) Shtedritski</a>,</span>
                  <span class="author-block">
                    <a href="https://chrirupp.github.io/" target="_blank">Christian Rupprecht</a>
                  </span>
                  </div>

                  <div class="is-size-6 publication-authors">
                    <span class="author-block text-muted">{jana, suny, chrisr}@robots.ox.ac.uk</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Visual Geometry Group, University of Oxford</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Jana-Z/highlight" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-justified">
        Given an image collection and, optionally, image descriptions, we automatically learn a Highlight to prompt images.
        Highlight outperforms a red circle by 15% on the RefCOCO, RefCOCO+ and RefCOCOg datasets, on average. 
      </h2>
      <!-- <img src="static/images/teaser.png" alt=""/> -->
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/teaser.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large-scale Vision-Language Models, such as CLIP, demonstrate impressive capabilities and have multiple applications, from text-to-image generation to zero-shot
            classification. Recent work has suggested that visual prompts, such as a red circle,
            can steer the vision encoder to the circled region. While such vision prompts have
            now been used in various applications, they might be model-specific and depend
            on the model learning these behaviours from its training data. Discovering and
            evaluating various prompts might not be feasible given different models, tasks, and
            datasets. In this paper, we propose Highlight, a method to learn a visual prompt
            that highlights a region in an image or refines a manually engineered visual prompt.
            Using our framework, we can learn to highlight in a supervised way using a dataset
            of text-image region pairs or in an unsupervised way using synthetic captions
            or images only. Highlight outperforms other visual prompts, prompt learning
            approaches, and compute-intensive methods that use ensembles of multiple models
            and visual prompts.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Methods -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <img src="static/images/method.png" alt=""/>
        <div class="content has-text-justified">
          <p>
            Highlight generates a visual prompt, which is then alpha-blended
            with object proposals in the image. To learn the visual prompt, we construct positive and negative
            pairs for each object using: (i) supervised text and bounding box pairs (e.g. from RefCOCO), (ii)
            unsupervised text and bounding box pairs obtained by captioning the bounding box, (iii) a visual
            representation of the object, e.g. a crop of the bounding box as in the Figure, or the original image,
            visually prompted with a red circle. Here, (ii) and (iii) are unsupervised in that they do not require
            manual text-image region annotations. The CLIP image and text encoder are kept frozen.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Methods -->

<!-- Results -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Results</h2>
            <div class="content has-text-justified">
              <p>
                Our supervised prompt outperforms prior works that use ensembles of several models and prompts, which require up to 6x more forward passes. Our unsupervised prompt outperforms all other single-prompts methods by 6.2% onaverage.
              </p>
            </div>
            <img class="blend-img-background center-image" src="static/images/tables/main.png" alt=""/>
            <div class="publication-authors">
              &dagger; as reported by <a target="_blank" href="https://arxiv.org/abs/2406.03303">Razei et al.</a> &ddagger; as reported by <a target="_blank" href="https://arxiv.org/abs/2304.06712">Shtedritski et al.</a>
            </div>
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <h3 class="title is-size-5">
                  Different Target Prompt Functions
                </h3>
                <img src="static/images/tables/table_different_f_star.png">
                <p class="has-text-justified subtitle is-size-6">
                  When training with our proposed image-image loss Highlight can be used to optimise existing prompts.
                  We see the biggest gains when using Crop, but the best performance when using Reverse Blur.
                </p>
              </div>
              <div class="item">
                <h3 class="title is-size-5">
                  Pretraining Improves Performance
                </h3>
                <img src="static/images/tables/table_pretraining.png">
                <p class="content has-text-justified subtitle is-size-6">
                  We compare training a Highlight from scratch with pretraining the Highlight to output a red circle initially.
                  We observe that the weaker the learning signal is, i.e. the weaker the supervsion is, the more important it is to pretrain Highlight meaningfully.
                </p>
              </div>
              <div class="item">
                <h3 class="title is-size-5">
                  Different Pretraining Shapes
                </h3>
                <img src="static/images/tables/pretrain.png">
                <p class="content has-text-justified subtitle is-size-6">
                    We train with our proposed Image-Image loss with different pretrain shapes and see that it is not only important to pretrain, but also colour and shape have an effect on the final result.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Results -->

<!-- Qual Results -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Optimised Highlights</h2>
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <img src="static/images/img-img-no-pre.png" />
                <h2 class="subtitle has-text-centered">Image-Image Training and No Pretraining.</h2>
              </div>
              <div class="item">
                <img src="static/images/img-img-pre.png" />
                <h2 class="subtitle has-text-centered">Image-Image Training and Red Circle Pretraining.</h2>
              </div>
              <div class="item">
                <img src="static/images/cap-img-no-pre.png" />
                <h2 class="subtitle has-text-centered">Synthetic Captions-Image Training and No Pretraining.</h2>
              </div>
              <div class="item">
                <img src="static/images/cap-img-pre.png" />
                <h2 class="subtitle has-text-centered">Synthetic Captions-Image Training and Red Circle Pretraining.</h2>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
</div>
</section>
<!-- End Qual Results -->

<!-- Results for other models -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">More Models</h2>
            <div class="content has-text-justified">
              <p>
                We train Highlight in the three different modes, (i)
                unsupervised im2im, using image-image pairs only, (ii) unsupervised t2im, using synthetic text-
                image pairs, and (iii) supervised t2im, using ground truth text-image pairs. Overall, we see that
                pretraining improves performance in the unsupervised image-image regime, and does not help or
                hurts performance when either ground-truth or unsupervised captions are used.
              </p>
            </div>
            <img class="blend-img-background center-image" src="static/images/tables/table_different_models.png" alt=""/>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
 <!-- End Results for other models -->



<!--BibTex citation -->
  <section class="section hero" id="BibTeX">
    <div class="container content">
      <h2 class="title">BibTeX</h2>
        <pre><code>@misc{zeller2024highlight,
          title={Highlight: Learning Visual Prompts for Vision-Language Models}, 
          author={Jana Zeller and Aleksandar Shtedritski and Christian Rupprecht},
          year={2024},
          eprint={<b>TODO</b>},
          archivePrefix={arXiv},
          primaryClass={cs.LG}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
